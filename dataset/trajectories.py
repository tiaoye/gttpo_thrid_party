import logging
import os
import math
import pickle
import numpy as np

import torch
from torch.utils.data import Dataset
from tqdm import tqdm
try:
    from dataset.intention_utils import rotate_to_xaxis,load_cluster,plot_cluster
except:
    from intention_utils import rotate_to_xaxis,load_cluster,plot_cluster

def gtppo_collate(data):
    '''
    collate function for GTPPO, data is generated by SGAN dataset. cause the dataset is generated by the same scene, so we just need to stack the data, but padding the data to the same pad length. Besides, the original loss mask data is always 1, so we ignore it.
    '''
    # (obs_seq_list, pred_seq_list, adj_list) = zip(*data)
    obs_seq_list = [d['obs_traj'] for d in data]
    pred_seq_list = [d['pred_traj'] for d in data]
    adj_list = [d['adj'] for d in data]
    if 'intention_cluster_label' in data[0]:
        intention_cluster_label_list = [d['intention_cluster_label'] for d in data]


    _len = [len(seq) for seq in obs_seq_list]
    max_ped_len = max(_len)

    # Input Data format: [ped_num, input_size, seq_len]
    # output Data format: batch, ped_num, seq_len, input_size
    batch_size = len(obs_seq_list)
    obs_len = obs_seq_list[0].shape[2]
    pred_len = pred_seq_list[0].shape[2]
    obs_traj = torch.zeros(batch_size, max_ped_len, obs_len, 2).float()
    pred_traj = torch.zeros(batch_size, max_ped_len, pred_len, 2).float()
    ped_mask = torch.zeros(batch_size, max_ped_len).bool()
    adj = torch.ones(batch_size, max_ped_len, max_ped_len, obs_len-1).float() * -1
    if 'intention_cluster_label' in data[0]:
        intention_cluster_label = torch.zeros(batch_size, max_ped_len).long()
    for i in range(batch_size):
        obs_traj[i, :_len[i], :, :] = obs_seq_list[i].permute(0, 2, 1)
        pred_traj[i, :_len[i], :, :] = pred_seq_list[i].permute(0, 2, 1)
        ped_mask[i, :_len[i]] = 1
        adj[i, :_len[i], :_len[i], :] = adj_list[i]
        if 'intention_cluster_label' in data[0]:
            intention_cluster_label[i, :_len[i]] = intention_cluster_label_list[i]

    out = {
        'obs_traj': obs_traj,
        'pred_traj': pred_traj,
        'ped_mask': ped_mask,
        'adj': adj
    }
    if 'intention_cluster_label' in data[0]:
        out['intention_cluster_label'] = intention_cluster_label
    return out

def seq_collate(data):
    (obs_seq_list, pred_seq_list, obs_seq_rel_list, pred_seq_rel_list,
     non_linear_ped_list, loss_mask_list) = zip(*data)

    _len = [len(seq) for seq in obs_seq_list]
    cum_start_idx = [0] + np.cumsum(_len).tolist()
    seq_start_end = [[start, end]
                     for start, end in zip(cum_start_idx, cum_start_idx[1:])]

    # Data format: batch, input_size, seq_len
    # LSTM input format: seq_len, batch, input_size
    obs_traj = torch.cat(obs_seq_list, dim=0).permute(2, 0, 1)
    pred_traj = torch.cat(pred_seq_list, dim=0).permute(2, 0, 1)
    obs_traj_rel = torch.cat(obs_seq_rel_list, dim=0).permute(2, 0, 1)
    pred_traj_rel = torch.cat(pred_seq_rel_list, dim=0).permute(2, 0, 1)
    non_linear_ped = torch.cat(non_linear_ped_list)
    loss_mask = torch.cat(loss_mask_list, dim=0)
    seq_start_end = torch.LongTensor(seq_start_end)
    out = [
        obs_traj, pred_traj, obs_traj_rel, pred_traj_rel, non_linear_ped,
        loss_mask, seq_start_end
    ]

    return tuple(out)


def read_file(_path, delim='\t'):
    data = []
    if delim == 'tab':
        delim = '\t'
    elif delim == 'space':
        delim = ' '
    with open(_path, 'r') as f:
        for line in f:
            line = line.strip().split(delim)
            line = [float(i) for i in line]
            data.append(line)
    return np.asarray(data)


def poly_fit(traj, traj_len, threshold):
    """
    return whether the trajectory is non-linear or not
    Input:
    - traj: Numpy array of shape (2, traj_len)
    - traj_len: Len of trajectory
    - threshold: Minimum error to be considered for non linear traj
    Output:
    - int: 1 -> Non Linear 0-> Linear
    """
    t = np.linspace(0, traj_len - 1, traj_len)
    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]
    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]
    if res_x + res_y >= threshold:
        return 1.0
    else:
        return 0.0

def cal_adj_martrix(curr_seq,curr_seq_rel,num_peds_considered,obs_len,max_num_ped=None):
    '''
    Args:
    - curr_seq: Numpy array of shape (num_peds, 2, seq_len)
    - curr_seq_rel: Numpy array of shape (num_peds, 2, seq_len)
    - num_peds_considered: Number of peds in the current sequence
    - obs_len: Number of time-steps in input trajectories
    - max_num_ped: Maximum number of peds in the current sequence
    Output:
    - adj: cosine values of the angle between velocity orientation of agent and the vector joining agent  and neighbor (max_num_ped, max_num_ped, obs_len-1)
    '''
    if max_num_ped is None:
        max_num_ped = num_peds_considered
    adj = np.ones((max_num_ped, max_num_ped, obs_len-1),dtype=np.float32) * (-1)
    for t in range(1,obs_len):
        for i in range(num_peds_considered):
            # guraantee that the velocity is not zero
            x = t
            velocity = curr_seq_rel[i,:,x]
            while x < curr_seq_rel.shape[2] and np.linalg.norm(velocity) == 0:
                velocity = curr_seq_rel[i,:,x]
                x += 1
            if x == curr_seq_rel.shape[2]:
                # print("Velocity is zero for all time-steps in the sequence")
                # print(curr_seq_rel[i,:,:])
                velocity = np.array([0,1])

            for j in range(num_peds_considered):
                if i == j:
                    adj[i,j,t-1] = 1.0
                else:
                    point_vector = curr_seq[j,:,t] - curr_seq[i,:,t]
                    assert np.linalg.norm(point_vector) != 0 and np.linalg.norm(velocity) != 0, "Zero vector "+str(point_vector)+" "+str(velocity)
                    adj[i,j,t-1] = np.dot(velocity,point_vector) / (np.linalg.norm(velocity) * np.linalg.norm(point_vector))
    return adj

class TrajectoryDataset(Dataset):
    """Dataloder for the Trajectory datasets"""
    def __init__(
        self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.002,
        min_ped=1, delim='\t', use_prepared_data=False, dump_prepared_data=False,
        use_intention_cluster=False
    ):
        """
        Args:
        - data_dir: Directory containing dataset files in the format
        <frame_id> <ped_id> <x> <y>
        - obs_len: Number of time-steps in input trajectories
        - pred_len: Number of time-steps in output trajectories
        - skip: Number of frames to skip while making the dataset
        - threshold: Minimum error to be considered for non linear traj
        when using a linear predictor
        - min_ped: Minimum number of pedestrians that should be in a seqeunce
        - delim: Delimiter in the dataset files
        - use_prepared_data: Use the prepared data if find in dir or not
        - dump_prepared_data: Dump the prepared data in the dir or not
        - use_intention_cluster: Use cluster intention label or not
        """
        super(TrajectoryDataset, self).__init__()
        self.data_dir = data_dir
        self.obs_len = obs_len
        self.pred_len = pred_len
        self.skip = skip
        self.threshold = threshold
        self.min_ped = min_ped
        self.seq_len = self.obs_len + self.pred_len
        self.delim = delim
        self.use_prepared_data = use_prepared_data
        self.dump_prepared_data = dump_prepared_data
        self.use_intention_cluster = use_intention_cluster

        if not self.use_prepared_data:
            self.prepare_data()
        else:
            self.load_prepared_data()
        
        if self.dump_prepared_data:
            self.save_prepared_data()
        
        if self.use_intention_cluster:
            self.cluster_intention()

    def prepare_data(self):
        all_files = os.listdir(self.data_dir)
        all_files = [os.path.join(self.data_dir, _path) for _path in all_files]
        # filter out the files that are not .txt
        all_files = [path for path in all_files if path.endswith('.txt')]
        num_peds_in_seq = []
        seq_list = []
        seq_list_rel = []
        loss_mask_list = []
        non_linear_ped = []
        # self.len_dict = {}
        self.adj_list = []
        for path in all_files:
            data = read_file(path, self.delim)
            frames = np.unique(data[:, 0]).tolist()
            frame_data = []
            for frame in frames:
                frame_data.append(data[frame == data[:, 0], :])
            num_sequences = int(
                math.ceil((len(frames) - self.seq_len + 1) / self.skip))

            for idx in tqdm(range(0, num_sequences * self.skip + 1, self.skip),desc="preprocessing "+path.split('\\')[-1]):
                curr_seq_data = np.concatenate(
                    frame_data[idx:idx + self.seq_len], axis=0)
                peds_in_curr_seq = np.unique(curr_seq_data[:, 1])
                curr_seq_rel = np.zeros((len(peds_in_curr_seq), 2,
                                         self.seq_len))
                curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))
                curr_loss_mask = np.zeros((len(peds_in_curr_seq),
                                           self.seq_len))
                num_peds_considered = 0
                _non_linear_ped = []
                for _, ped_id in enumerate(peds_in_curr_seq):
                    curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] ==
                                                 ped_id, :]
                    curr_ped_seq = np.around(curr_ped_seq, decimals=4)
                    pad_front = frames.index(curr_ped_seq[0, 0]) - idx
                    pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1
                    if pad_end - pad_front != self.seq_len:
                        continue
                    curr_ped_seq = np.transpose(curr_ped_seq[:, 2:]) # 2xT
                    curr_ped_seq = curr_ped_seq
                    # Make coordinates relative
                    rel_curr_ped_seq = np.zeros(curr_ped_seq.shape)
                    rel_curr_ped_seq[:, 1:] = \
                        curr_ped_seq[:, 1:] - curr_ped_seq[:, :-1]
                    _idx = num_peds_considered
                    curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq
                    curr_seq_rel[_idx, :, pad_front:pad_end] = rel_curr_ped_seq
                    # Linear vs Non-Linear Trajectory
                    _non_linear_ped.append(
                        poly_fit(curr_ped_seq, self.pred_len, self.threshold))
                    curr_loss_mask[_idx, pad_front:pad_end] = 1
                    num_peds_considered += 1

                if num_peds_considered > self.min_ped:
                    non_linear_ped += _non_linear_ped
                    num_peds_in_seq.append(num_peds_considered)
                    loss_mask_list.append(curr_loss_mask[:num_peds_considered])
                    seq_list.append(curr_seq[:num_peds_considered])
                    seq_list_rel.append(curr_seq_rel[:num_peds_considered])
                    self.adj_list.append(torch.from_numpy(cal_adj_martrix(curr_seq,curr_seq_rel,num_peds_considered,self.obs_len)).type(torch.float))

            # from collections import Counter
            # self.len_dict[path] = Counter([len(x) for x in seq_list])
        self.num_seq = len(seq_list)
        seq_list = np.concatenate(seq_list, axis=0) # N x 2 x T
        seq_list_rel = np.concatenate(seq_list_rel, axis=0) # N x 2 x T
        loss_mask_list = np.concatenate(loss_mask_list, axis=0) # N x T
        non_linear_ped = np.asarray(non_linear_ped) # N

        # Convert numpy -> Torch Tensor
        self.obs_traj = torch.from_numpy(
            seq_list[:, :, :self.obs_len]).type(torch.float) # N x 2 x Tobs
        self.pred_traj = torch.from_numpy(
            seq_list[:, :, self.obs_len:]).type(torch.float) # N x 2 x Tpred
        self.obs_traj_rel = torch.from_numpy(
            seq_list_rel[:, :, :self.obs_len]).type(torch.float) # N x 2 x Tobs
        self.pred_traj_rel = torch.from_numpy(
            seq_list_rel[:, :, self.obs_len:]).type(torch.float) # N x 2 x Tpred
        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float) # N x T
        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float) # N
        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist() # seq_nums
        self.seq_start_end = [
            (start, end)
            for start, end in zip(cum_start_idx, cum_start_idx[1:])
        ]

    def cluster_intention(self):
        '''make intention cluster label'''
        # if use intention_cluster
        kmeans = load_cluster("data/ethucy_sgan/raw/all_data/cluster_result_4.pkl")
        obs_traj_rot, pred_traj_rot = rotate_to_xaxis(self.obs_traj, self.pred_traj)
        intention_cluster_label = torch.from_numpy(kmeans.predict(pred_traj_rot[:,:,-1])) # N
        # print(intention_cluster_label.shape)
        # plot_cluster(pred_traj_rot, intention_cluster_label, subplot=(2,2), plot_curves=False)
        # make one-hot vector
        self.intention_cluster_label = intention_cluster_label.long()



    def load_prepared_data(self):
        """Load the prepared data from the pickle file."""
        prepared_data_path = os.path.join(self.data_dir, "prepared_data.pkl")
        if not os.path.exists(prepared_data_path):
            print("Prepared data not found. Preparing data...")
            self.prepare_data()
            return

        with open(prepared_data_path, 'rb') as f:
            data = pickle.load(f)
        self.obs_traj = data['obs_traj']
        self.pred_traj = data['pred_traj']
        self.obs_traj_rel = data['obs_traj_rel']
        self.pred_traj_rel = data['pred_traj_rel']
        self.non_linear_ped = data['non_linear_ped']
        self.loss_mask = data['loss_mask']
        self.seq_start_end = data['seq_start_end']
        self.num_seq = data['num_seq']
        self.adj_list = data['adj_list']

    def save_prepared_data(self):
        """Dump the prepared data into a pickle file."""
        prepared_data_path = os.path.join(self.data_dir, "prepared_data.pkl")
        data = {
            'obs_traj': self.obs_traj,
            'pred_traj': self.pred_traj,
            'obs_traj_rel': self.obs_traj_rel,
            'pred_traj_rel': self.pred_traj_rel,
            'non_linear_ped': self.non_linear_ped,
            'loss_mask': self.loss_mask,
            'seq_start_end': self.seq_start_end,
            'num_seq': self.num_seq,
            'adj_list': self.adj_list
        }
        with open(prepared_data_path, 'wb') as f:
            pickle.dump(data, f)
        print("Prepared data saved to {}".format(prepared_data_path))


    def __len__(self):
        return self.num_seq

    def __getitem__(self, index):
        start, end = self.seq_start_end[index]
        out = {
            'obs_traj': self.obs_traj[start:end, :],
            'pred_traj': self.pred_traj[start:end, :],
            'adj': self.adj_list[index],
        }
        if self.use_intention_cluster:
            out['intention_cluster_label'] = self.intention_cluster_label[start:end]
        return out


if __name__ == '__main__':
    from torch.utils.data import DataLoader
    file_path = "data/ethucy_sgan/raw/all_data"
    dst = TrajectoryDataset(data_dir=file_path,use_prepared_data=True,dump_prepared_data=True,use_intention_cluster=True)
    loader = DataLoader(dst, batch_size=5, shuffle=True, num_workers=0, collate_fn=gtppo_collate)
    for i, out in enumerate(tqdm(loader)):
        obs_traj, pred_traj, ped_mask, adj = out['obs_traj'], out['pred_traj'], out['ped_mask'], out['adj']
        intention_cluster_label = out['intention_cluster_label']
        # print(obs_traj.shape[1])
        continue
        # print(obs_traj.shape)
        # print(pred_traj.shape)
        # print(ped_mask.shape)
        # print(adj.shape)
        # print(intention_cluster_label.shape)
        break